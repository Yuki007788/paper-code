{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "396959f3",
   "metadata": {},
   "source": [
    "This notebook reproduces the main results and figures of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a53314",
   "metadata": {},
   "source": [
    "We compared CP decomposition to Phasik method through simulation models and use a widely adopted ODE-based budding yeast cell-cycle model inspired by Chen et al. (2004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import tellurium as te\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import non_negative_parafac\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SBMLファイルを読み込んでODEに変換\n",
    "r = te.loadSBMLModel('https://www.ebi.ac.uk/biomodels/model/download/BIOMD0000000056.2?filename=BIOMD0000000056_url.xml')\n",
    "result = r.simulate(0, 250, 500)\n",
    "r.plot(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b17841",
   "metadata": {},
   "source": [
    "I gain data that Time*Concentration about each materials and I can confirm \"oscillation\" noted by J.J.tyson et al. (1991).\n",
    "We do not assume access to the true physical PPI network, instead, pairwise interaction strength is approximated from protein activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4145b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=result.colnames)\n",
    "df.to_csv('simulation_result2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正規化と変換、幾つデータあるか確認\n",
    "time = df[\"time\"].values\n",
    "X = df.drop(columns=[\"time\"])   \n",
    "X_norm = X / X.max(axis=0)                   \n",
    "concentration_matrix = X_norm.values.T        \n",
    "\n",
    "proteins, Time_steps = concentration_matrix.shape\n",
    "print(\"Proteins\", proteins, \"Time steps:\", Time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd5bab",
   "metadata": {},
   "source": [
    "We calculate N* N *T third-order tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf6b03",
   "metadata": {},
   "source": [
    "Although, a detailed reaction or interaction graph is available for the simulated cell-cycle model, we intentionally do not use the corresponding \"true\" interaction mask in the main experiments.\n",
    "\n",
    "This is because such a mask would encode near-complete information about the underlying regulatory structure, effectively providing oracle-level prior knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = concentration_matrix[:, None, :]* concentration_matrix[None, :, :]\n",
    "mask = np.ones((proteins, proteins))\n",
    "np.fill_diagonal(mask, 1) ##\n",
    "tensor = tensor * mask[:, :, None]\n",
    "print(f'Tensor shape: {tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0378c03",
   "metadata": {},
   "source": [
    "We intentionally adopt a minimal co-activity representation based on outer products of protein activities, which avoids explicil network assumptions while preserving temporal structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5e8c1",
   "metadata": {},
   "source": [
    "We branch into analyses.\n",
    "I combine the concentration information in csv with a static network.\n",
    "First, we operate Phasik (exsisting method). The flow is that we fold third-order tensor into the list of N * N and compute distance matrices between snapshots and then, clustering by Ward's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f865eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phasik\n",
    "N, _, T = tensor.shape\n",
    "flat_snapshots = tensor.transpose(2, 0, 1).reshape(T, -1)\n",
    "distances_condensed = pdist(flat_snapshots, metric='euclidean')\n",
    "distance_matrix = squareform(distances_condensed)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(distance_matrix, cmap='viridis_r') \n",
    "plt.title('Distance Matrix (Euclidean Norm)')\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('Time t\\'')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e218cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#クラスタリング\n",
    "Z = linkage(squareform(distance_matrix), method='ward')\n",
    "\n",
    "n_phases = 4\n",
    "phasik_labels = fcluster(Z, t = n_phases, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dendrogram\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    no_labels=True,\n",
    "    color_threshold=Z[-(n_phases)+1, 2])\n",
    "plt.axhline(y=Z[-(n_phases)+1, 2], color='r', linestyle='--',\n",
    "            label='Cut for 4 clusters')\n",
    "plt.title(\"Hierarchical Clustering of Time Axis (Ward's linkage)\")\n",
    "plt.xlabel(\"Time index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f7e50",
   "metadata": {},
   "source": [
    "We use Ward's linkage with Euclidean distance, which is consistent with the variance-minimizing objective of Ward clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CP分解\n",
    "\n",
    "rank = 4\n",
    "weights, factors = non_negative_parafac(tensor, rank=rank, init='random', random_state=42)\n",
    "Time_factors = factors[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f644d1",
   "metadata": {},
   "source": [
    "We fix the CP rank based on biological prior knowledge rather than optimizing rank selection heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd66070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phasik results\n",
    "fig = plt. figure(figsize=(12, 8))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 2])\n",
    "\n",
    "ax0 = plt.subplot(gs[0])\n",
    "im = ax0.imshow(phasik_labels.reshape(1, -1), aspect='auto', cmap='tab10', interpolation= 'nearest')\n",
    "ax0.set_title('Phasik (Discrete)', fontsize=14)\n",
    "ax0.set_yticks([])\n",
    "ax0.set_xlabel('Time')\n",
    "\n",
    "values = np.unique(phasik_labels)\n",
    "colors = [im.cmap(im.norm(value)) for value in values]\n",
    "patches = [mpatches.Patch(color=colors[i], label=f\"Phase {i+1}\") for i in range(len(values))]\n",
    "ax0.legend(handles=patches, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "#CP decomposition results\n",
    "ax1 = plt.subplot(gs[1], sharex=ax0)\n",
    "for i in range(rank):\n",
    "    ax1.plot(Time_factors[:, i], label=f'Component {i+1}', linewidth=2)\n",
    "\n",
    "ax1.set_title('Tensor Decomposition (Continuous)', fontsize=14)\n",
    "ax1.set_ylabel('Activity Level (Arbitrary Unit)')\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6cc8c8",
   "metadata": {},
   "source": [
    "Phasik aims at discrete phase segmentation, whereas CP decomposition extracts continuous latent modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636bbbb",
   "metadata": {},
   "source": [
    "CP分解はSVDと違って、一意にきまらない。解析結果に再現性を持たせるために初期値を変えてプロットさせる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6062f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 4\n",
    "n_repeats = 10\n",
    "best_runs = []\n",
    "\n",
    "print(\"Running decomposition checks...\")\n",
    "for i in range(n_repeats):\n",
    "    weights, factors = non_negative_parafac(tensor, rank=rank, init='random', random_state=i)\n",
    "    \n",
    "    recon = tl.cp_to_tensor((weights, factors))\n",
    "    error = tl.norm(tensor - recon)/ tl.norm(tensor)\n",
    "    \n",
    "    best_runs.append((error, factors[2]))\n",
    "\n",
    "best_runs.sort(key=lambda x: x[0])\n",
    "top_5_runs = best_runs[:5]\n",
    "\n",
    "print(f\"Top 1 Error: {top_5_runs[0][0]:.6f}\")\n",
    "print(f\"Top 5 Error: {top_5_runs[4][0]:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for comp_i in range(rank):\n",
    "    plt.subplot(2, 2, comp_i + 1)\n",
    "    \n",
    "    # トップ5の試行結果を重ねる\n",
    "    for run_idx in range(5):\n",
    "        time_factor = top_5_runs[run_idx][1]\n",
    "        \n",
    "        plt.plot(time_factor[:, comp_i], alpha=0.5, linewidth=2)\n",
    "        \n",
    "    plt.title(f'Component {comp_i + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed19b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_factors = top_5_runs[0][1] # (Time, Rank)\n",
    "\n",
    "# 並べ替え後の波形を格納するリスト（最初は基準データを入れる）\n",
    "aligned_time_factors = [reference_factors]\n",
    "\n",
    "# 2. 残りの4つの試行を、基準データに合わせて並べ替える\n",
    "for i in range(1, 5):\n",
    "    target_factors = top_5_runs[i][1]\n",
    "    \n",
    "    # 類似度行列を作成\n",
    "    similarity_matrix = np.zeros((rank, rank))\n",
    "    for r in range(rank): # Ref\n",
    "        for t in range(rank): # Target\n",
    "            vec_ref = reference_factors[:, r]\n",
    "            vec_tar = target_factors[:, t]\n",
    "            norm = np.linalg.norm(vec_ref) * np.linalg.norm(vec_tar)\n",
    "            similarity_matrix[r, t] = np.dot(vec_ref, vec_tar) / (norm + 1e-9)\n",
    "            \n",
    "    # ハンガリアン法で最適な割り当てを計算\n",
    "    row_ind, col_ind = linear_sum_assignment(-similarity_matrix)\n",
    "    \n",
    "    # 並べ替え実行\n",
    "    aligned_target = target_factors[:, col_ind]\n",
    "    aligned_time_factors.append(aligned_target)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for comp_i in range(rank):\n",
    "    ax = plt.subplot(2, 2, comp_i + 1)\n",
    "    \n",
    "    # 5回の試行結果を重ね描き\n",
    "    for run_idx in range(5):\n",
    "        wave = aligned_time_factors[run_idx][:, comp_i]\n",
    "        ax.plot(wave, linewidth=2, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'Component {comp_i + 1}', fontweight='bold')\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8313a85",
   "metadata": {},
   "source": [
    "CP分解した後のヒートマップと各フェーズにおける寄与が大きい上位5つのタンパク質を調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 4\n",
    "n_repeats = 10\n",
    "reorder_indices = [2, 0, 1, 3]\n",
    "\n",
    "best_error = np.inf\n",
    "best_factors = None\n",
    "\n",
    "for seed in range(n_repeats):\n",
    "    weights, factors = non_negative_parafac(tensor, rank=rank, init=\"random\", random_state=seed)\n",
    "    recon = tl.cp_to_tensor((weights, factors))\n",
    "    error = tl.norm(tensor - recon)/ tl.norm(tensor)\n",
    "\n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_factors = factors\n",
    "\n",
    "csv_path = \"/Users/katsuyukiyasebe/Desktop/卒研/simulation_result2.csv\"\n",
    "df_names = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "protein_factor = best_factors[0]  # (N_proteins, rank)\n",
    "\n",
    "if len(df_names.index) == protein_factor.shape[0]:\n",
    "    protein_names = df_names.index\n",
    "elif len(df_names.columns) == protein_factor.shape[0]:\n",
    "    protein_names = df_names.columns\n",
    "else:\n",
    "    protein_names = [f\"Protein {i+1}\" for i in range(protein_factor.shape[0])]\n",
    "\n",
    "protein_factor_reordered = protein_factor[:, reorder_indices]\n",
    "xticklabels = [f\"Comp {i+1}\" for i in reorder_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 14))\n",
    "sns.set(font_scale=0.8)\n",
    "sns.heatmap(\n",
    "    protein_factor_reordered,\n",
    "    xticklabels=xticklabels,\n",
    "    yticklabels=protein_names,\n",
    "    cmap=\"viridis\",\n",
    "    annot=False,\n",
    ")\n",
    "plt.title(\"Spatial Structure: Protein Importance (Reordered)\")\n",
    "plt.xlabel(\"Components (Phases)\")\n",
    "plt.ylabel(\"Proteins\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for col_idx in reorder_indices:\n",
    "    col_data = protein_factor[:, col_idx]\n",
    "    top_indices = np.argsort(np.abs(col_data))[::-1][:5]\n",
    "\n",
    "    print(f\"\\nComponent {col_idx+1}:\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"  - {protein_names[idx]}: {col_data[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f8d43",
   "metadata": {},
   "source": [
    "それぞれのフェーズで必要なタンパク質が正しく抽出できているとChen(2004)の論文から確認できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2c241",
   "metadata": {},
   "source": [
    "最後に、ノイズ耐性のテストをする。今はODE由来のデータを用いているため綺麗なデータであるが、ノイズを混ぜることでどこまで崩れずに波形を取り出せるか。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810137e",
   "metadata": {},
   "source": [
    "We evaluate how closely the temporal factors extracted from noisy data resemble those obtained from the noise-free data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing Noise-free data\")\n",
    "\n",
    "# 念のためランクを定義\n",
    "rank = 4\n",
    "\n",
    "# クリーンデータでの分解\n",
    "w_true, f_true = non_negative_parafac(tensor, rank=rank, init='random', n_iter_max=1000, random_state=0)\n",
    "true_time_factor = f_true[2] # 時間因子\n",
    "\n",
    "print(f\"Noise-free Shape: {true_time_factor.shape}\") \n",
    "\n",
    "noise_levels = [0.0, 0.1, 0.3, 0.5, 0.8, 1.0, 1.5, 2.0]\n",
    "similarity_scores = []\n",
    "n_trials_per_level = 5\n",
    "\n",
    "print(\"\\nRunning Noise Robustness Test...\")\n",
    "\n",
    "for nl in noise_levels:\n",
    "    scores = []\n",
    "    print(f\"Testing Noise Level: {nl} ...\", end=\" \")\n",
    "    \n",
    "    # 信号の強さ（標準偏差）\n",
    "    sig_std = np.std(tensor)\n",
    "    \n",
    "    for trial in range(n_trials_per_level):\n",
    "        # ノイズ生成\n",
    "        noise = np.random.normal(0, sig_std * nl, tensor.shape)\n",
    "        noisy_tensor = tensor + noise\n",
    "        noisy_tensor = np.maximum(noisy_tensor, 0) # 非負制約\n",
    "        \n",
    "        # 分解実行\n",
    "        try:\n",
    "            w, f = non_negative_parafac(noisy_tensor, rank=rank, init='random', n_iter_max=500, tol=1e-6)\n",
    "            pred_time_factor = f[2]\n",
    "            \n",
    "            # マッチング処理\n",
    "            sim_matrix = np.zeros((rank, rank))\n",
    "            for i in range(rank): \n",
    "                for j in range(rank): \n",
    "                    vec_true = true_time_factor[:, i]\n",
    "                    vec_pred = pred_time_factor[:, j]\n",
    "                    \n",
    "                    # ベクトルの正規化と内積\n",
    "                    norm_true = np.linalg.norm(vec_true)\n",
    "                    norm_pred = np.linalg.norm(vec_pred)\n",
    "                    \n",
    "                    if norm_pred < 1e-9: # ゼロ除算回避\n",
    "                        sim = 0\n",
    "                    else:\n",
    "                        sim = np.dot(vec_true, vec_pred) / (norm_true * norm_pred + 1e-9)\n",
    "                    \n",
    "                    sim_matrix[i, j] = sim\n",
    "            \n",
    "            # ハンガリアン法でベストペアを探す\n",
    "            row_ind, col_ind = linear_sum_assignment(-sim_matrix)\n",
    "            \n",
    "            # 平均スコア\n",
    "            avg_sim = sim_matrix[row_ind, col_ind].mean()\n",
    "            scores.append(avg_sim)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[Error at Level {nl}, Trial {trial}] {e}\")\n",
    "            scores.append(0)\n",
    "            \n",
    "    # 平均スコアを記録\n",
    "    if len(scores) > 0:\n",
    "        mean_score = np.mean(scores)\n",
    "    else:\n",
    "        mean_score = 0\n",
    "        \n",
    "    similarity_scores.append(mean_score)\n",
    "    print(f\"Mean Score: {mean_score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(noise_levels, similarity_scores, marker='o', linewidth=2, color='blue')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='High Reliability (0.9)')\n",
    "plt.axhline(y=0.7, color='orange', linestyle='--', label='Acceptable (0.7)')\n",
    "\n",
    "plt.title(\"Noise Robustness of NTF on Cell Cycle Data\")\n",
    "plt.xlabel(\"Noise Level (Relative to Signal Std)\")\n",
    "plt.ylabel(\"Reconstruction Accuracy (Cosine Similarity)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff7fb3",
   "metadata": {},
   "source": [
    "Noise Level 1.0 の数値に注目したい。Noise Level 1.0 とは：信号と同じ強さのノイズが入っている状態である（S/N比 $\\approx$ 0dB）。肉眼で見ると、元の波形がかなりガタガタで見えにくいレベルである。結果 (Score: 0.9881)：それでも、抽出された波形は 98% 元の形を保っている。CP分解は、データを少数の成分の積で表現しようとする。ランダムなガウスノイズは、この「綺麗な積の形」にはフィットしないため、計算過程で「誤差」として自動的に捨てられる。Therefore, low-rank constraint suppresses unstructured noise components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514deb4",
   "metadata": {},
   "source": [
    "In this study, we demonstrate that low-rank tensor decomposition can stably extract continuous temporal modes from dynamic interaction data, even in the absence of explicit network topology and under substantial noise.\n",
    "These results suggest that tensor-based representations provide a robust alternative to phase-based segmentation approaches for analyzing complex biologial dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9e67a",
   "metadata": {},
   "source": [
    "I evaluates the algorithmic stability of our Non-negative CP decomposition over 50 random initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 4\n",
    "n_repeats = 50          # 30〜50推奨\n",
    "top_k = 10              # top1〜top10\n",
    "eps = 1e-12\n",
    "def rel_recon_error(X, Xhat):\n",
    "    return float(tl.norm(X - Xhat) / (tl.norm(X) + eps))\n",
    "\n",
    "def normalize_cols(A, mode=\"l2\"):\n",
    "    A = np.array(A, dtype=float)\n",
    "    if mode == \"zscore\":\n",
    "        mu = A.mean(axis=0, keepdims=True)\n",
    "        sd = A.std(axis=0, keepdims=True) + eps\n",
    "        return (A - mu) / sd\n",
    "    # default: L2 normalize each column\n",
    "    denom = np.sqrt((A**2).sum(axis=0, keepdims=True)) + eps\n",
    "    return A / denom\n",
    "\n",
    "def cosine_sim_matrix(A, B):\n",
    "    # A,B: (T, R) column-wise vectors\n",
    "    A2 = normalize_cols(A, mode=\"l2\")\n",
    "    B2 = normalize_cols(B, mode=\"l2\")\n",
    "    return A2.T @ B2  # (R,R)\n",
    "\n",
    "def match_components_time_factor(C_ref, C_run, use_abs=True, zscore_before=True):\n",
    "    \"\"\"\n",
    "    Match components of C_run to C_ref with Hungarian algorithm.\n",
    "    Returns:\n",
    "      perm: permutation indices for run components aligned to ref\n",
    "      sims: cosine similarities (matched), shape (R,)\n",
    "    \"\"\"\n",
    "    if zscore_before:\n",
    "        C_ref_n = normalize_cols(C_ref, mode=\"zscore\")\n",
    "        C_run_n = normalize_cols(C_run, mode=\"zscore\")\n",
    "    else:\n",
    "        C_ref_n = C_ref\n",
    "        C_run_n = C_run\n",
    "\n",
    "    S = cosine_sim_matrix(C_ref_n, C_run_n)  # (R,R)\n",
    "    if use_abs:\n",
    "        S_use = np.abs(S)\n",
    "    else:\n",
    "        S_use = S\n",
    "\n",
    "    # Hungarian\n",
    "    row_ind, col_ind = linear_sum_assignment(-S_use)\n",
    "\n",
    "    sims = S_use[row_ind, col_ind]\n",
    "    return col_ind, sims\n",
    "\n",
    "\n",
    "# Multi-start runs\n",
    "\n",
    "runs = []\n",
    "print(\"Running multi-start NNCP ...\")\n",
    "for seed in range(n_repeats):\n",
    "    weights, factors = non_negative_parafac(tensor, rank=rank, init=\"random\", random_state=seed)\n",
    "    recon = tl.cp_to_tensor((weights, factors))\n",
    "    err = rel_recon_error(tensor, recon)\n",
    "\n",
    "    # store time factor only (mode-2 assuming tensor is (i,j,t))\n",
    "    C = np.array(factors[2], dtype=float)  # (T, R)\n",
    "    runs.append({\n",
    "        \"seed\": seed,\n",
    "        \"rel_err\": err,\n",
    "        \"C\": C\n",
    "    })\n",
    "\n",
    "runs_sorted = sorted(runs, key=lambda d: d[\"rel_err\"])\n",
    "top_runs = runs_sorted[:top_k]\n",
    "\n",
    "print(f\"Best rel_err  : {top_runs[0]['rel_err']:.6f} (seed={top_runs[0]['seed']})\")\n",
    "print(f\"Top{top_k} rel_err: {top_runs[-1]['rel_err']:.6f} (seed={top_runs[-1]['seed']})\")\n",
    "\n",
    "C_ref = top_runs[0][\"C\"]\n",
    "for d in top_runs:\n",
    "    perm, sims = match_components_time_factor(C_ref, d[\"C\"], use_abs=True, zscore_before=True)\n",
    "    d[\"perm\"] = perm\n",
    "    d[\"matched_sims\"] = sims\n",
    "    d[\"mean_sim\"] = float(np.mean(sims))\n",
    "    d[\"min_sim\"] = float(np.min(sims))\n",
    "\n",
    "C_ref_all = runs_sorted[0][\"C\"]\n",
    "for d in runs_sorted:\n",
    "    perm, sims = match_components_time_factor(C_ref_all, d[\"C\"], use_abs=True, zscore_before=True)\n",
    "    d[\"mean_sim\"] = float(np.mean(sims))\n",
    "    d[\"min_sim\"] = float(np.min(sims))\n",
    "\n",
    "errs_top = [d[\"rel_err\"] for d in top_runs]\n",
    "mean_sims_top = [d[\"mean_sim\"] for d in top_runs]\n",
    "\n",
    "errs_all = [d[\"rel_err\"] for d in runs_sorted]\n",
    "mean_sims_all = [d[\"mean_sim\"] for d in runs_sorted]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Panel 1: reconstruction error distribution\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.hist(errs_all, bins=20, alpha=0.6, label=\"all runs\")\n",
    "y_offset = 1.5\n",
    "ax1.scatter(errs_top, np.full_like(errs_top, y_offset), marker=\"x\", s=80, label=f\"top{top_k}\")\n",
    "ax1.set_title(\"Relative reconstruction error\")\n",
    "ax1.set_xlabel(r\"$||T-\\hat{T}||_F / ||T||_F$\")\n",
    "ax1.set_ylabel(\"count\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.hist(mean_sims_all, bins=20, alpha=0.6, label=\"all runs\")\n",
    "y_offset = 1.5\n",
    "ax2.scatter(mean_sims_top, np.full_like(mean_sims_top, y_offset), marker=\"x\", s=80, label=f\"top{top_k}\")\n",
    "ax2.set_title(\"Mean component similarity (time factors)\")\n",
    "ax2.set_xlabel(\"mean |cosine| after Hungarian matching\")\n",
    "ax2.set_ylabel(\"count\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop runs summary (top_k):\")\n",
    "for k, d in enumerate(top_runs, start=1):\n",
    "    print(f\"{k:2d}: seed={d['seed']:3d}  rel_err={d['rel_err']:.6f}  mean_sim={d['mean_sim']:.3f}  min_sim={d['min_sim']:.3f}\")\n",
    "\n",
    "# -----------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "for comp in range(rank):\n",
    "    plt.subplot(2, 2, comp + 1)\n",
    "    for d in top_runs:\n",
    "        C = d[\"C\"]\n",
    "        Cn = normalize_cols(C, mode=\"zscore\")\n",
    "        Cn_aligned = Cn[:, d[\"perm\"]]  # align columns\n",
    "        plt.plot(Cn_aligned[:, comp], alpha=0.5, linewidth=2)\n",
    "    plt.title(f\"Component {comp+1} (aligned, z-scored)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0975ef9",
   "metadata": {},
   "source": [
    "The left panel shows that the distribution of the relative reconstruction errors, as you can see, the top runs form a tight cluster at the lower end, indicating that the algorithm consistently converges to the same global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560f24a2",
   "metadata": {},
   "source": [
    "The right panel displays that the mean cosine similarity of the temporal factors compared to the best run. The values are concentrated near 1.0, which confirms that the extracted components are highly reproducible and robust against initialization noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282a2a9",
   "metadata": {},
   "source": [
    "As shown in these plots, the waveforms for each component overlap almost perfectly. This suggests that the interpretation of the components is reliable and not an artifact of a specific initilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bce58",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
